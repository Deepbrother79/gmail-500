name: Gmail500 Scraper

on:
  # Scheduling: ogni ora (totalmente gratuito!)
  schedule:
    - cron: '0 * * * *'  # Ogni ora (alle :00)
    # Altre opzioni:
    # - cron: '0 */2 * * *'   # Ogni 2 ore
    # - cron: '0 */6 * * *'   # Ogni 6 ore
    # - cron: '0 9,21 * * *'  # Alle 9:00 e 21:00 UTC

  # Permetti esecuzione manuale
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Cache Puppeteer browsers
        uses: actions/cache@v3
        with:
          path: ~/.cache/puppeteer
          key: ${{ runner.os }}-puppeteer-${{ hashFiles('package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-puppeteer-

      - name: Cache node modules
        uses: actions/cache@v3
        with:
          path: node_modules
          key: ${{ runner.os }}-node-${{ hashFiles('package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-node-

      - name: Install dependencies
        run: npm ci --prefer-offline --no-audit

      - name: Run scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE: ${{ secrets.SUPABASE_SERVICE_ROLE }}
          URL_TABLE: ${{ secrets.URL_TABLE }}
          URL_SCARP_1: ${{ secrets.URL_SCARP_1 }}
          URL_SCARP_1_JSON: ${{ secrets.URL_SCARP_1_JSON }}
          URL_SCARP_2: ${{ secrets.URL_SCARP_2 }}
          URL_SCARP_2_JSON: ${{ secrets.URL_SCARP_2_JSON }}
        run: node github-actions-scraper.js

      - name: Report status
        if: always()
        run: |
          if [ $? -eq 0 ]; then
            echo "✅ Scraping completed successfully"
          else
            echo "❌ Scraping failed"
            exit 1
          fi
